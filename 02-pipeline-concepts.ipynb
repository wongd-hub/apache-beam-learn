{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Concepts\n",
    "\n",
    "Some key terms:\n",
    "- `Pipeline`: encapsulates your entire data processing task from start to finish (read in, transform, write out). All Beam river programs must create a Pipeline.\n",
    "- `PCollection`: represents a distributed data set that the Beam pipeline operates on. It can either be bounded (fixed source/flat file) or unbounded (data stream). Reading in input at the start of the `Pipeline` generally creates a `PCollection`, but you can also create one from in-memory data. `PCollections` are the inputs and outputs for each step in your pipeline.\n",
    "- `PTransform`: a data processing operation or step in your `Pipeline`. Every `PTransform` takes one or more `PCollection` objects as an input, then returns 0 or more `PCollection`s.\n",
    "    - This can be any change, filter, group, analyze process. Each `PTransform` creates a new output without modifying the input collection.\n",
    "- `I/O transforms`: Beam comes with various \"IOs\", `PTransforms` that read or write data to various systems.\n",
    "\n",
    "Generally, a Beam pipeline will:\n",
    "- Create a `Pipeline` object and set pipeline execution options (including the *Pipeline Runner* - Direct (in-mem) or Dataflow (Google Dataflow))\n",
    "- Create an initial `PCollection` using IOs or from in-memory data\n",
    "- Apply `PTransforms` to each `PCollection` wher necessary\n",
    "- Use IOs to write the final `PCollection` to an external source\n",
    "- Run the `Pipeline` using the designated *Pipeline Runner*\n",
    "\n",
    "Some more Beam terms:\n",
    "- `Aggregation`: computing a value from multiple input elements\n",
    "- `User-defined function (UDF)`: some Beam operations allow you to run user-defined code as a way to configure a transform\n",
    "- `Schema`: language-independent type definition for a `PCollection`\n",
    "- `SDK`: language-specific library that lets pipeline authors build transforms, construct pipelines, and submit them to a runner\n",
    "- `Window`: A `PCollection` can be subdivided into windows based on the timestamps of the individual elements. Windows enable grouping operations overcollections that grow over time\n",
    "- `Watermark`: a guess as to when all data in a certain window is expected to have arrived\n",
    "- `Trigger`: determins when to aggregate the results of each window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating pipelines\n",
    "\n",
    "The `Pipeline` encapsulates all the data and steps in your data pocessing task. You start by constructing a `Pipeline` object and using that object as the basis for the pipeline's operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass explicitly specified parameters this way:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "beam_options = PipelineOptions(\n",
    "    flags  = [], # This must be here otherwise it will pull in sys.argv https://stackoverflow.com/a/75265512\n",
    "    runner = 'DirectRunner' # or 'DataflowRunner\n",
    "    # project       = 'my-proj',         # GCP Dataflow option\n",
    "    # job_name      = 'unique-job-name', # GCP Dataflow option\n",
    "    # temp_location = 'gs://my-bucket',  # GCP Dataflow option\n",
    ")\n",
    "\n",
    "with beam.Pipeline(options=beam_options) as p:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring pipeline options\n",
    "\n",
    "You can read options from the command line by simply setting up `PipelineOptions` (can be empty), to have it take in command line args, do not define the `flags` argument. This will interpret command-line arguments as: \n",
    "\n",
    "```shell\n",
    "--<option>=<value>\n",
    "```\n",
    "\n",
    "You can also create custom options by creating a `PipelineOptions` subclass:\n",
    "\n",
    "```python\n",
    "class MyOptions(PipelineOptions):\n",
    "    @classmethod\n",
    "    def _add_argparse_args(cls, parser):\n",
    "        parser.add_argument(\n",
    "            '--input'\n",
    "            default='gs://dataflow-samples/shakespear/kinglear.txt',\n",
    "            help='The file path for the input text to process'\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            '--output',\n",
    "            required=True\n",
    "        )\n",
    "```\n",
    "\n",
    "Or by simply parsing the custom options with `argparse`:\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "def main(argv=None, save_main_session=True):\n",
    "    \n",
    "    # Define new arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--input',\n",
    "        dest='input',\n",
    "        default='gs://dataflow-samples/shakespear/kinglear.txt',\n",
    "        help='The file path forthe input text to process'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output',\n",
    "        dest='output',\n",
    "        required=True\n",
    "    )\n",
    "\n",
    "    # Split arguments into the args defined above, and all other args \n",
    "    # (which we assume go directly into `PipelineOptions`)\n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "    # Run pipeline args through PipelineOptions\n",
    "    pipeline_options = PipelineOptions(pipeline_args)\n",
    "\n",
    "    # Use save_main_session if our workflow relies on the global context \n",
    "    # (e.g. a module defined at the global level)\n",
    "    pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n",
    "\n",
    "    with beam.Pipeline(options=pipeline_options) as p:\n",
    "\n",
    "        lines = (\n",
    "            p\n",
    "            # Using an additional arg directly in pipeline\n",
    "            | 'Read' >> ReadFromText(known_args.input)\n",
    "            | beam.Filter(lambda line: line != \"\")\n",
    "        )\n",
    "\n",
    "        output = lines | 'Write' >> WriteToText(known_args.output)\n",
    "\n",
    "        result = p.run()\n",
    "        result.wait_until_finish()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: 02-pipeline-concepts.py [-h] [--input INPUT] [--output OUTPUT]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help       show this help message and exit\n",
      "  --input INPUT    The file path for the input text to process\n",
      "  --output OUTPUT\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python source/02-pipeline-concepts.py --help # argparse automatically builds help docco when adding args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python source/02-pipeline-concepts.py \\\n",
    "    --output \"output/agatha-count\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache-beam-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
