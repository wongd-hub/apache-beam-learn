{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Concepts\n",
    "\n",
    "Some key terms:\n",
    "- `Pipeline`: encapsulates your entire data processing task from start to finish (read in, transform, write out). All Beam river programs must create a Pipeline.\n",
    "- `PCollection`: represents a distributed data set that the Beam pipeline operates on. It can either be bounded (fixed source/flat file) or unbounded (data stream). Reading in input at the start of the `Pipeline` generally creates a `PCollection`, but you can also create one from in-memory data. `PCollections` are the inputs and outputs for each step in your pipeline.\n",
    "- `PTransform`: a data processing operation or step in your `Pipeline`. Every `PTransform` takes one or more `PCollection` objects as an input, then returns 0 or more `PCollection`s.\n",
    "    - This can be any change, filter, group, analyze process. Each `PTransform` creates a new output without modifying the input collection.\n",
    "- `I/O transforms`: Beam comes with various \"IOs\", `PTransforms` that read or write data to various systems.\n",
    "\n",
    "Generally, a Beam pipeline will:\n",
    "- Create a `Pipeline` object and set pipeline execution options (including the *Pipeline Runner* - Direct (in-mem) or Dataflow (Google Dataflow))\n",
    "- Create an initial `PCollection` using IOs or from in-memory data\n",
    "- Apply `PTransforms` to each `PCollection` wher necessary\n",
    "- Use IOs to write the final `PCollection` to an external source\n",
    "- Run the `Pipeline` using the designated *Pipeline Runner*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
